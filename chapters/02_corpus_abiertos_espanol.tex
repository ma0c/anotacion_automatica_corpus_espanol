\chapter{Revisión de recursos abiertos para el español}

La investigación en tecnologías del habla depende fuertemente en recursos de lenguaje de alta calidad y gran tamaño diseñados para la lengua específica. Los recursos abiertos de este tipo son más escasos en comparación con recursos de pago y la creación de dichos recursos para tareas como reconocimiento y síntesis de voz es costosa, por lo que muchos investigadores optan por crear conjuntos de datos pequeños y enfocados a su investigación.

Para lenguajes diferentes al inglés, la diferencia en disponibilidad de dichos recursos es aún mayor. Este capítulo presenta una compilación de recursos abiertos para tareas relacionados con tecnologías de voz para la lengua española, describiendo cada recurso y proponiendo posibles aplicaciones para cada corpus. Para seleccionar los recursos aquí incluídos se usaron como criterios la visibilidad, disponibilidad y el impacto.

% Veo que todo hace parte de la introducción del capítulo. MACB
% \section{Preámbulo}

Los modelos utilizados regularmente para procesamiento de voz requieren cantidades masivas de datos para lograr buenos resultados. En reconocimiento de voz, sistemas como Listen-Attend-Spell de Google \cite{Chan2016} fue entrenado utilizando 2000 horas aumentadas 20 veces con técnicas de aumento de datos hasta tener un corpus de 40000 horas. Otros modelos del tipo secuencia a secuencia usan 12500 horas de grabación \cite{Chiu2018}. Otros recursos reconocidos en la literatura como  TIMIT \cite{TIMIT} o Swichboard \cite{Switchboard} tienen una duración menor pero su anotación es a nivel fonético o de palabra.

Aunque los esfuerzos para reducir los requerimientos por grandes volúmenes de datos anotados usando aprendizaje semi supervisado \cite{AmazonSemiSupervised} y no supervisado \cite{ZeroResources}, estas aproximaciones requieren una gran cantidad de datos no supervisados.

Estas complicaciones se hacen mas evidentes en lenguas distintas al inglés donde los recursos son limitados en tamaño, variedad y nivel de anotación \cite{HernndezMena2017}. De igual manera, en comparación con el inglés que  tiene lineas base de investigación desde hace mas de dos décadas con corpus como TIMIT \cite{TIMIT}, Switchboard \cite{Switchboard} o Fisher \cite{Fisher}, para el español las líneas bases para investigación no están claramente definidas usualmente usan recursos multilingues y más limitados \cite{euronews_multilingual,librilight}.

La disponibilidad de los recursos para la investigación es un aspecto fundamental en el desarrollo de prototipos, algoritmos y técnicas. La tabla \ref{tab:resources_by_langauge} compara el n\'umero de recursos de licencias abiertas y de pago en los dos catálogos más comunes de recursos de lenguaje, el Linguistic Data Consortium (LDC) y la European Language Resources Association (ELRA), respectivamente. Podemos ver que el inglés tiene cinco veces más recursos que el español y el mandarín \cite{HernndezMena2017}.

\input{tablas/02_01_corpus_por_lengua}

% Moví este párrafo arriba. MACB
%Este capítulo muestra una revisión de los recursos de licencia abierta para tareas de procesamiento de voz en español, usando como criterios la visibilidad, disponibilidad y el impacto, indicando que existen otros recursos no considerados por falta de los criterios mencionados previamente.


\section{Recursos abiertos para el español}

Esta sección menciona los recursos de lenguaje abiertos para la lengua española con licencias abiertas y distribuidos abiertamente en la web. Un resumen se presenta en la tabla \ref{tab:open_source_spanish_corpus} organizada por fecha de publicación; mostrando características importantes de cada conjunto de datos, como la licencia de distribución, el nivel de anotación, la duración, las condiciones de grabación, el dialecto y la fuente de las grabaciones.

En la tabla \ref{tab:open_source_spanish_corpus} se utilizan las siguientes abreviaciones:  CC BY-SA 4.0 Creative Commons Share A Like v4.0 license, LDC Non-Members LDC User LDC User Agreement for Non-Members, para dialectos: Mexicano (MX), Peninsular (ES), Argentino (AR), Chileno (CH), Peruano (PE), Colombiano (CO), Puerto Riqueño (PR), Venezolano (VE). 

\input{tablas/02_02_corpus_abiertos_espanol}

\subsection{DIMEx100}

DIMEx100 \cite{Pineda2004DIMEx100:Spanish} es un corpus oral diseñado y grabado por el Instituto de Investigaciones en Matemáticas Aplicadas y en Sistemas IIMAS de la Universidad Nacional Autónoma de México UNAM en 2004. DIMEx100 utiliza textos del Corpus 230 \cite{Corpus230}, un corpus escrito fonéticamente equilibrado para el español. Este proyecto fue anotado usando un nuevo diccionario fonético para el español mexicano llamado MEXBET, que mejora la representación de los alófonos del español mexicano en comparación con otros alfabetos fonéticos multilingües como SAMPA y WordNET \cite{mexbet}. El proceso de grabación para DIMEx100 se realizó en un estudio de grabación, con bajo nivel de ruido, con el micrófono ubicado a una distancia homogénea de los altavoces. Se seleccionaron 100 hablantes entre 16 y 36 años siendo 50 hombres y 50 mujeres. La edad media de los hablantes fue de 23 años y todos los hablantes hablan el dialecto mexicano.

Este es un corpus con 5.6 horas de duración, con equilibrio fonético y de género y conformado por 5010 declaraciones con anotaciones fonéticas. Este corpus se puede usar para el reconocimiento de palabras aisladas o como recurso inicial para la construcción corpus más grandes. Sin embargo, todos los locutores de este corpus usan el dialecto mexicano, lo que podría generar inconvenientes al realizar tareas donde los locutores usen otro dialecto.

\subsection{Heroico}

Heroico \cite{heroico} es un corpus hablado publicado en 2006 por John Morgan en el Departamento de Lenguas Extranjeras (DFL) y el Centro para el Aprendizaje de Idiomas Mejorado por la Tecnología (CTELL). Todas las grabaciones se llevaron acabo en El Heroico Colegio Militar y la Academia Militar Mexicana en México. Este corpus tiene un desequilibrio de género y se anota a nivel de declaración. El corpus de HEROICO está compuesto por 102 hablantes que responden abiertamente a un conjunto de 143 preguntas y leen 75 expresiones de un conjunto de datos compuesto por 205 frases cortas y 519 oraciones simples de notas académicas de la escuela secundaria.

La distribución del corpus Heroico también incluye el subcorpus USMA, registrado en 1997 que está compuesto por 206 expresiones fijas leídas por 18 hablantes nativos y no nativos diferentes. La duración total del corpus es de 13 horas. Todas las grabaciones se realizaron con computadoras de escritorio, el ruido está presente en varias grabaciones.

Los corpus Heroico y USMA combinados tienen 13 horas de audio, y parte del corpus es habla espontánea en entornos ruidosos y diferentes potencias de señal, lo que permite que este corpus entrene modelos para un reconocimiento de voz robusto. Además, algunos hablantes en el corpus de la USMA no son hablantes nativos de español y hay diferentes dialectos de español que también dan variabilidad para los modelos independientes del hablante.

\subsection{Proyecto CIEMPIESS}

El Corpus de Investigación en Español de México del Posgrado de Ingeniería Eléctrica y Servicio Social de la Universidad Nacional Autónoma de México CIEMPIES-UNAM es un proyecto iniciado en 2012 con el objetivo de desarrollar y compartir herramientas gratuitas y de código abierto para el procesamiento del habla en el idioma español. El proyecto no se limita a la creación de corpus sino a la investigación en tecnologías de habla aplicadas al idioma español \cite{CIEMPIESS-Webpage}. Para la anotación de corpus, el proyecto CIEMPIESS utiliza estudiantes de pregrado para realizar anotaciones manuales. Toda la segmentación y anotación se realiza utilizando herramientas de código abierto. Adicionalmente, el corpus final es distribuido bajo la licencia abierta Creative Commons Share A Like versión 4. Este proyecto ha publicado varios corpus a lo largo de los años, incluido el corpus original de CIEMPIESS \cite {CIEMPIESS}, CIEMPIESS Light \cite {CIEMPIESS-LIGHT}, CIEMPIESS Balance \cite {CIEMPIESS-BALANCE}, CIEMPIESS Experimentation \cite {CIEMPIESS-Experimentation} y LibriVoxSpanish \cite {LibriVox-Spanish} que se describen a continuación.

\subsubsection{CIEMPIESS}

El Corpus de Investigación en Español de México del Posgrado de Ingeniería Eléctrica y Servicio Social CIEMPIESS \cite{CIEMPIESS} fue publicado en 2014 por el Departamento de Procesamiento Digital de Señales de la Universidad Nacional Autónoma de México, con el objetivo de crear un corpus hablado para el reconocimiento automático de voz. Todas las grabaciones fueron extraídas de diversos programas de radio emitidos por la Universidad, en total se seleccionaron 43 programas de una hora, los cuales fueron segmentados manualmente y sólo los segmentos de un solo locutor fueron seleccionados; segmentos con sonidos de fondo como música o interferencias fueron removidos. Dando la naturaleza del programa de radio, este corpus contiene discurso continuo y es desequilibrado de género, con 77.86\% grabaciones masculinas y solo 22.14\% grabaciones femeninas. El corpus completo consta de 16.717 grabaciones y tiene una duración total de 17 horas. El proceso de anotación se realizó usando el alfabeto fonético MEXBET vocales tónicas \cite{mexbet}. La anotación se realizó a nivel de palabras con formato TextGrid \cite{TextGrids}.

\subsubsection{CIEMPIESS Light}

CIEMPIES Light \cite{CIEMPIESS-LIGHT} se publicó después de dos años de experimentación con el CIEMPIESS Corpus. El equipo de CIEMPIESS recibió muchos comentarios y se lanzó una nueva versión del corpus original. En esta revisión se cambió el formato de distribución de archivos SPH a archivos WAV, también se eliminó una pequeña porción de grabaciones reportadas como problemáticas y se añadieron otras para compensar las eliminadas, añadiendo en total una hora m\'as de audio. Esta nueva distribución es compatible con herramientas ASR modernas como Kaldi y CMU Sphinx. La mayoría de las grabaciones originales se conservaron, pero otras se reemplazaron. 

Este corpus puede considerarse como la versión dos del corpus original de CIEMPIESS, pero mejorado para trabajar con sistemas ASR modernos El corpus está organizado por género y locutor (53 hombres y 34 mujeres en total) y anotado a nivel de declaración. 

\subsubsection{CIEMPIESS Balance}

CIEMPIESS Balance \cite{CIEMPIESS-BALANCE} fue creado considerando que el corpus de CIEMPIESS tiene un desequilibrio de género, se creó un corpus de nueva corpus a partir de grabaciones del mismo programa de radio para equilibrar el corpus de CIEMPIESS LIGHT. Este corpus está compuesto por 18 horas y 20 minutos donde 12 horas y 40 segundos son de hablantes mujeres y 5 horas y 40 minutos son de hablantes masculinos, para un total de 36 horas aproximadamente por cada género.


\subsubsection{CIEMPIES EXPERIMENTATION}

CIEMPIES Experimentación \cite{CIEMPIESS-Experimentation} es un conjunto de tres corpus distribuidos en uno solo, con un corpus para alófonos de equilibrio fonético para el español mexicano, un corpus con s\'olo mujeres hablantes y uno diseñado para ser un corpus de prueba estándar.

CIEMPIESS EXPERIMENTATION - COMPLEMENTARY es un corpus fonéticamente equilibrado para palabras aisladas que contiene una hora de audio anotado usando MEXBET 29 y MEXBET 66, dos anotaciones fonéticas que consideran diferentes alófonos para el idioma español. Este corpus contiene grabaciones de 10 locutores masculinos y 10 locutores femeninos y fue creado para mejorar los motores de reconocimiento de voz que no encontraron ocurrencias de alófonos específicos al entrenar modelos acústicos.

CIEMPIES EXPERIMENTATION  - FEM es un corpus con sólo locutoras femeninas, que contiene 13 horas y 54 minutos de grabaciones Hay un total de 21 locutoras diferentes: 16 hablantes de México, y 5 hablantes de otros dialectos (venezolano, argentino, salvadoreño, dominicano, y desconocido)

CIEMPIES EXPERIMENTATION  - TEST es un corpus con equilibrio de género diseñado para probar aplicaciones de habla. Cuenta con un total de 8 horas y 8 minutos de grabaciones de 10 locutoras mujeres y 10 locutores masculinos.

\subsubsection {Librivox Spanish}

Librivox Spanish \cite{LibriVox-Spanish} es un corpus en español basado en grabaciones abiertas subidas al sitio Librivox \footnote{http://librivox.org}, todos los audiolibros son de dominio público y parte del Proyecto Guttenberg \footnote{https://gutenberg.org} o liberados al dominio público. El corpus fue anotado manualmente por estudiantes de pregrado como parte de su requerimiento de servicio social en la Universidad Nacional Autónoma de México. Tiene un total de 73 horas de audio, balanceado por género con 60 horas de hablantes nativos de español y el resto de hablantes no nativos. Debido a su naturaleza de creación colectiva, las grabaciones tienen una calidad diferente; en la mayoría de los casos, las grabaciones se realizaron en un entorno silencioso y utilizando micrófonos de computadora normales.

Como este corpus se anota manualmente y las grabaciones de audio originales son de dominio público, el corpus en sí se puede utilizar como un corpus de prueba para la segmentación automática de las grabaciones originales. Además, el corpus tiene hablantes con diferentes dialectos, lo que hace que sea apropiado crear sistemas de reconocimiento de voz independientes del locutor.

\subsection{M-AILABS}

M-AILABS Speech Dataset \cite{M-AILABS} es un corpus hablado creado por Imdat Solak utilizando recursos disponibles abiertamente de LibriVox y Project Guttenberg. El corpus era multilingüe, incluido alemán, inglés estadounidense e inglés británico, español, italiano, ruso ucraniano, francés y polaco.

El subcorpus en español tiene 108 horas de duración y está  dividido en tres subcorpus: un corpus femenino de 10 horas, grabado por una locutora mexicana, un corpus masculino de 72 horas, grabado por un argentino y un español, y un corpus mixto de 25 horas, grabado por varios hablantes no identificados. Todas las grabaciones incluidas en este corpus también pertenecen al proyecto LibriVox.

Para la segmentación se utilizó una segmentación basada en silencios, determinando los valores mínimos de toda la grabación en decibeles y cortando en segmentos de intensidad menor. Posteriormente utilizando un servicio de reconocimiento automático de voz en cada segmento se evalúan manualmente los resultados obtenidos contra el texto original, dejando al final una transcripción corregida por un humano.



\subsection{Google TTS Latin American}

El corpus de Crowdsourcing Latin American Spanish for Low-Resource Text-to-Speech \cite{googleTTSLatinAmericanSpanishCorpus} fue creado por Google Research y el Laboratoire de Sciences Cognitives et Psycholinguistique y Graduate School of Engineering de la Universidad de Tokio. El corpus se diseñó para tener varios dialectos con alta calidad para los sistemas de texto a voz de América Latina. Este corpus incluye 6 subcorpus para dialectos argentino, chileno, colombiano, peruano, puertorriqueño y venezolano, y un total de 174 hablantes y 37,7 horas de audio. Las oraciones se seleccionaron con base en un sistema de conversación para el español mexicano. Posteriormente las oraciones se adaptaron a cada dialecto en particular y solo 30 se mantuvieron como canónicas las cuales todos los locutores debían grabar.

Las grabaciones se realizaron en una cabina vocal portátil con un micrófono de condensador en un entorno cercano al silencio.

\subsection{Common Voice}

La fundación de software Mozilla creó a mediados del 2017 una plataforma de crowdsourcing para recopilar recursos de voz. Estos recursos hacen parte del proyecto Deep Speech Project, basado en la propuesta de Baidu Deep Speech \cite{deepspeeh}. Posterioromente la iniciativa se consolidó como un proyecto independiente llamado Common Voice \cite{Common-Voice}.

Usando una plataforma web \footnote{https://commonvoice.mozilla.org/}, los usuarios graban frases cortas y validan las grabaciones de otros usuarios. La última versión  incluyó 60 idiomas, incluidos entre ellos el inglés, alemán, francés y chino. El idioma español cuenta con 579 horas de audio.

Considerando que el corpus es grabado y validado por voluntarios, las condiciones de grabación no son homogéneas y pueden contener errores de anotación o ruido de fondo.

\subsection{Vox Populli}

Vox Populli \cite{voxpopulli} es un corpus publicado por Facebook anotando automáticamente las grabaciones y traducciones realizadas en el parlamento europeo. El corpus está separado en 16 lenguajes, con 160 horas de grabaciones anotadas a nivel de declaración de 305 locutores distintos y 4400 horas no anotadas para el idioma español. 

El procesamiento de las horas anotadas consistió en el uso de las transcripciones manuales de las sesiones. Como estas transcripciones tenían problemas de alineación, sobre cada audio, que corresponde a un párrafo se realizó una segmentación por silencios de duración máxima de 20 segundos. La duración aproximada de cada párrafo original es de 197 segundos. Los nuevos subsegmentos fueron anotados automáticamente con un software de reconocimiento automático de voz. Se comparó el resultado obtenido por el software reconocimiento automático de voz con la anotación esperada en la transcripción manual y los resultados individuales con precisión superior al 80\% fueron conservados.



\section{Herramientas existentes para la anotación automática}
% \section{Existing tools}

Muchos Alineadores Forzados de código abierto utilizan investigación teórica para materializar la alineación de recursos existentes. Basada en la categorización mencionada previamente, se agrupan alineadores existentes en las siguientes categorías

\begin{itemize}
    \item Pliegues Dinámicos Temporales
    \item Modelos Ocultos de Markov
    \item Redes Neuronales Artificiales
\end{itemize}
% Open source forced aligners uses theoretical research to create tools to materialize alignment on existing resources. Based on previously mentioned categorization the existing aligners are grouped in the categories: Dynamic Time Warping, Hidden Markov Models and Artifical Neural Networks.

\subsection{Pliegues Dinámicos Temporales}
% \subsection{Dynamic Time Warping}

Para DTW la idea principal es generar señales de voz artificiales utilizando software de tipo grafema a fonema, también conocido como texto a vos (Text To Speech TTS), y luego aliear la señal de entrada con la generada artificialmente.

% For DTW the main idea is to generate an artificial speech using Grapheme to Phoneme  software and then align the input speech with the artificially generated wave

\textbf{Aeneas}.

Aneneas \cite{aeneas} es un software desarrollado por Alberto Pettarin para Read Beyond, un software de audio libros, liberado bajo licencia GNU Affero General Public Licence versión 3 (AGPL v3) en 2015, con varias actualizaciones hasta 2017. Para la generación de la señal artificial, Aeneas usa eSpeak \cite{espeak}, otro software licenciado bajo la licencia GNU Public Licence (GPL) por the Free Software Foundation. eSpeak en su versión original soporta más de 28 lenguajes, incluyendo Inglés y Español
% Aeneas \cite{aeneas} uses espeak \cite{espeak} to generate the base generates speech wave. Then align the input word using dynamic programming.

\subsection{Modelos Ocultos de Markov}
% \subsection{Hidden Markov Models}

Los Modelos Ocultos de Markov usan una serie de algoritmos para parametrizar los modelos, entre los cuales se destacan el algoritmo de Viterbi para la decodificación \cite{Forney1973TheAlgorithm} y el algoritmo Baum-Welch para la estimación inicial de parámetros.

Este conjunto de algoritmos son implementados por paquetes especializados para el reconocimiento automático del habla, como  HTK \cite{Young1994ThePhilosophy}, Julius \cite{LeeEurospeechEngine} y CMU Sphinx \cite{Lee1990AnSystem}. Estas implementaciones abiertas de algoritmos son usadas por algunas herramientas mostradas a continuación.
% Work with HMM uses a basic set of algorithms where the Viterbi algorithm \cite{Forney1973TheAlgorithm} and the Baum Welch Algorithm. These set of algorithms are implemented for ASR in frameworks and toolkits like HTK \cite{Young1994ThePhilosophy}, Julius \cite{LeeEurospeechEngine} and CMU Sphinx \cite{Lee1990AnSystem}.

\textbf{MAUS}
El Segmentador Automático de Munich (Munich AUtomatic Segmentation) desarrollado por el Instituto de Fonética y procesamiento de señales de la Universidad de Munich está construido usando herramientas de decodificación de HTK y BALLON un sistema de grafemas a fonemas también desarrollado por la  universidad de Munich. Este paquete fue desarrollado en 1994 por Uwe Reichel y Florian Schiel licenciado para propósitos no comerciales y académicos. \cite{WesenickAPPLYINGPRONUNCIATION}. 
% The Munich Automatic Segmentation \cite{WesenickAPPLYINGPRONUNCIATION} is a build on top of HTK and BALLON a Grapheme to Phoneme Suite created by Uwe Reichel and uses a hybrid approach between DTW and HMM

\textbf{SPPAS}
SPPAS es un anotador automático y analizador de habla es una herramienta de computación científica desarrollada para proveer un análisis fonético robusto y confiable \cite{Bigi2016ASPPAS}. Fue creada por Brigitte Bigi  en el Laboratorio del habla y la lengua (Laboratoire Parole et Langage) de Francia, licenciado con la licencia GPL v3, utiliza Julius para el procesamiento de los Modelos Ocultos de Markov.
% SPPAS \cite{Bigi2016ASPPAS} is a suite for automated for automated annotation and speech analysis created by Brigitte Bigi at Laboratoire Parole et Langage in France and uses Julius for HMM processing and Viterbi Algorithm

\textbf{Prosodylab Aligner}
Prosodylab Aligner \cite{Gorman2011Prosodylab-aligner:Speech} fue creado en el Laboratorio de Prosodia (Prosody Lab) en la Universidad de Mcgill en Canada, por Kyle Gorman y está compuesta de una serie de herramientas y scripts para crear alineaciones usando HTK usando monófonos para entrenar sus modelos.
% Prosodylab Aligner \cite{Gorman2011Prosodylab-aligner:Speech} was created at Prosody Lab in Mcgill University by Kyle Gorman and are composed by a set of tools of scripts to create alignment using HTK as backend using mono-phones to train its models

\subsection{Redes Neuronales Artificiales}
% \subsection{Artificial Neural Networks}

Los anotadores de código abierto basados en redes neuronales utilizan Kaldi \cite{Povey_ASRU2011}, un paquete diseñado por Daniel Povey en el instituto universitario John Hopkings, y licenciado bajo la licencia Apache 2.
% For ANN the Kaldi toolkit \cite{Povey_ASRU2011} is used to improve development times.   

\textbf{Gentle Forced Aligner}

Gentle \cite{gentle} es un alineador forzado construido a partir de un modelo de bigramas y conceptos de programación dinámica para alinear el audio.
% Gentle \cite{gentle} is a FA build on top of Kaldi. The main approach is to use a web server to evaluate an input speech. The model uses dynamic programming to align audio using a bigram model

\textbf{The Montreal Forced Aligner}

El Alineador Forzado de Montreal, fue creado en la Universidad de Mcgill en el laboratorio de prosidia \cite{McAuliffe2017MontrealKaldi}, licenciado con la licencia MIT  como una versión mejorada de Prosylab Aligner, utilizando el corpus GlobalPhone  para crear modelos trifónicos que mejoran el rendimiento con respecto al alineador anterior.
% The Montreal Forced Aligner \cite{McAuliffe2017MontrealKaldi} was created at Prosody Lab in Mcgill University as the evolution for Prosodylab Aligner. It uses Kaldi as backend and Globalphone to create a triphone model that improves performance comparing Prosodylab Aligner